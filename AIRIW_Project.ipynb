{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruchitha2912/Doc2Graph/blob/main/AIRIW_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVVje9QlkOq",
        "outputId": "7e79d2b5-7715-497b-d98a-cc4215e06a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.0 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "\n",
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_file_path) as pdf_document:\n",
        "        for page_number in range(pdf_document.page_count):\n",
        "            page = pdf_document.load_page(page_number)\n",
        "            page_text = page.get_text()\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Function to replace \"theta\" with \"ti\" and \"sigma\" with \"tt\" using regular expressions\n",
        "def replace_misinterpretations(text):\n",
        "    text = re.sub(r'\\b(Ɵ)\\b', 'ti', text)\n",
        "    text = re.sub(r'(\\b\\w*)Ɵ(\\w*\\b)', r'\\1ti\\2', text)\n",
        "    text = re.sub(r'\\b(Ʃ)\\b', 'tt', text)\n",
        "    text = re.sub(r'(\\b\\w*)Ʃ(\\w*\\b)', r'\\1tt\\2', text)\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "pdf_file_path = \"Natural language processing.pdf\"  # Replace with the path to your PDF file\n",
        "extracted_text = extract_text_from_pdf(pdf_file_path)\n",
        "processed_text = replace_misinterpretations(extracted_text)\n",
        "print(processed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1grKMa8PlvBY",
        "outputId": "c471bba6-8720-47b7-c948-636700bb3fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing (NLP) is an interdisciplinary subfield of computer \n",
            "science and linguistics. It is primarily concerned with giving computers the ability to support and \n",
            "manipulate human language. It involves processing natural language datasets, such as text \n",
            "corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most \n",
            "recently, neural network-based) machine learning approaches. The goal is a computer capable \n",
            "of \"understanding\" the contents of documents, including the contextual nuances of the language \n",
            "within them. The technology can then accurately extract information and insights contained in the \n",
            "documents as well as categorize and organize the documents themselves. \n",
            "Challenges in natural language processing frequently involve speech recognition, natural-\n",
            "language understanding, and natural-language generation. \n",
            " \n",
            "History \n",
            "Natural language processing has its roots in the 1940s. Already in 1940, Alan Turing published \n",
            "an article titled \"Computing Machinery and Intelligence\" which proposed what is now called \n",
            "the Turing test as a criterion of intelligence, though at the time that was not articulated as a \n",
            "problem separate from artificial intelligence. The proposed test includes a task that involves the \n",
            "automated interpretation and generation of natural language. \n",
            " \n",
            "Symbolic NLP (1950s – early 1990s)[edit] \n",
            "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: \n",
            "Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), \n",
            "the computer emulates natural language understanding (or other NLP tasks) by applying those \n",
            "rules to the data it confronts. \n",
            "• \n",
            "1950s: The Georgetown experiment in 1954 involved fully automatic translation of \n",
            "more than sixty Russian sentences into English. The authors claimed that within \n",
            "three or five years, machine translation would be a solved problem. However, real \n",
            "progress was much slower, and after the ALPAC report in 1966, which found that \n",
            "ten-year-long research had failed to fulfill the expectations, funding for machine \n",
            "translation was dramatically reduced. Little further research in machine translation \n",
            "was conducted in America (though some research continued elsewhere, such as \n",
            "Japan and Europe)  until the late 1980s when the first statistical machine \n",
            "translation systems were developed. \n",
            "• \n",
            "1960s: Some notably successful natural language processing systems developed in \n",
            "the 1960s were SHRDLU, a natural language system working in restricted \"blocks \n",
            "worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian \n",
            "psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using \n",
            "almost no information about human thought or emotion, ELIZA sometimes provided a \n",
            "startlingly human-like interaction. When the \"patient\" exceeded the very small \n",
            "knowledge base, ELIZA might provide a generic response, for example, responding \n",
            "to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's \n",
            "successful work on natural language was demonstrated with a vocabulary of \n",
            "only twenty words, because that was all that would fit in a computer memory at the \n",
            "time. \n",
            "• \n",
            "1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", \n",
            "which structured real-world information into computer-understandable data. Examples \n",
            "are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin \n",
            "(Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units \n",
            "(Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY). \n",
            "• \n",
            "1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus \n",
            "areas of the time included research on rule-based parsing (e.g., the development \n",
            "of HPSG as a computational operationalization of generative grammar), morphology \n",
            "(e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within \n",
            "Centering Theory) and other areas of natural language understanding (e.g., in \n",
            "the Rhetorical Structure Theory). Other lines of research were continued, e.g., the \n",
            "development of chatterbots with Racter and Jabberwacky. An important development \n",
            "(that eventually led to the statistical turn in the 1990s) was the rising importance of \n",
            "quantitative evaluation in this period.  \n",
            " \n",
            "Statistical NLP (1990s–2010s) \n",
            "Up until the 1980s, most natural language processing systems were based on complex sets of \n",
            "hand-written rules. Starting in the late 1980s, however, there was a revolution in natural \n",
            "language processing with the introduction of machine learning algorithms for language \n",
            "processing. This was due to both the steady increase in computational power (see Moore's law) \n",
            "and the gradual lessening of the dominance of Chomskyan theories of linguistics \n",
            "(e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus \n",
            "linguistics that underlies the machine-learning approach to language processing  \n",
            "• \n",
            "1990s: Many of the notable early successes on statistical methods in NLP occurred \n",
            "in the field of machine translation, due especially to work at IBM Research, such \n",
            "as IBM alignment models. These systems were able to take advantage of existing \n",
            "multilingual textual corpora that had been produced by the Parliament of Canada and \n",
            "the European Union as a result of laws calling for the translation of all governmental \n",
            "proceedings into all official languages of the corresponding systems of government. \n",
            "However, most other systems depended on corpora specifically developed for the \n",
            "tasks implemented by these systems, which was (and often continues to be) a major \n",
            "limitation in the success of these systems. As a result, a great deal of research has \n",
            "gone into methods of more effectively learning from limited amounts of data. \n",
            "• \n",
            "2000s: With the growth of the web, increasing amounts of raw (unannotated) \n",
            "language data has become available since the mid-1990s. Research has thus \n",
            "increasingly focused on unsupervised and semi-supervised learning algorithms. \n",
            "Such algorithms can learn from data that has not been hand-annotated with the \n",
            "desired answers or using a combination of annotated and non-annotated data. \n",
            "Generally, this task is much more difficult than supervised learning, and typically \n",
            "produces less accurate results for a given amount of input data. However, there is an \n",
            "enormous amount of non-annotated data available (including, among other things, \n",
            "the entire content of the World Wide Web), which can often make up for the inferior \n",
            "results if the algorithm used has a low enough time complexity to be practical. \n",
            " \n",
            "Neural NLP (present)[edit] \n",
            "In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by \n",
            "a multi-layer perceptron (with a single hidden layer and context length of several words trained \n",
            "on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with \n",
            "co-authors.  \n",
            "In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors \n",
            "applied a simple recurrent neural network with a single hidden layer to language modelling and in \n",
            "the following years he went on to develop Word2vec. In the 2010s, representation \n",
            "learning and deep neural network-style (featuring many hidden layers) machine learning \n",
            "methods became widespread in natural language processing. That popularity was due partly to a \n",
            "flurry of results showing that such techniques can achieve state-of-the-art results in many natural \n",
            "language tasks, e.g., in language modeling and parsing. This is increasingly important in \n",
            "medicine and healthcare, where NLP helps analyze notes and text in electronic health \n",
            "records that would otherwise be inaccessible for study when seeking to improve care[16] or protect \n",
            "patient privacy.  \n",
            " \n",
            "Approaches: Symbolic, statistical, neural networks[edit] \n",
            "Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with \n",
            "a dictionary lookup, was historically the first approach used both by AI in general and by NLP in \n",
            "particular:[18][19] such as by writing grammars or devising heuristic rules for stemming. \n",
            "Machine learning approaches, which include both statistical and neural networks, on the other \n",
            "hand, have many advantages over the symbolic approach: \n",
            "• \n",
            "both statistical and neural networks methods can focus more on the most common \n",
            "cases extracted from a corpus of texts, whereas the rule-based approach needs to \n",
            "provide rules for both rare cases and common ones equally. \n",
            "• \n",
            "language models, produced by either statistical or neural networks methods, are \n",
            "more robust to both unfamiliar (e.g. containing words or structures that have not \n",
            "been seen before) and erroneous input (e.g. with misspelled words or words \n",
            "accidentally omitted) in comparison to the rule-based systems, which are also more \n",
            "costly to produce. \n",
            "• \n",
            "the larger such a (probabilistic) language model is, the more accurate it becomes, in \n",
            "contrast to rule-based systems that can gain accuracy only by increasing the amount \n",
            "and complexity of the rules leading to intractability problems. \n",
            "Although rule-based systems for manipulating symbols were still in use in 2020, they have \n",
            "become mostly obsolete with the advance of LLMs in 2023. \n",
            "Before that they were commonly used: \n",
            "• \n",
            "when the amount of training data is insufficient to successfully apply machine \n",
            "learning methods, e.g., for the machine translation of low-resource languages such \n",
            "as provided by the Apertium system, \n",
            "• \n",
            "for preprocessing in NLP pipelines, e.g., tokenization, or \n",
            "• \n",
            "for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge \n",
            "extraction from syntactic parses. \n",
            " \n",
            "Common NLP tasks[edit] \n",
            "The following is a list of some of the most commonly researched tasks in natural language \n",
            "processing. Some of these tasks have direct real-world applications, while others more \n",
            "commonly serve as subtasks that are used to aid in solving larger tasks. \n",
            "Though natural language processing tasks are closely intertwined, they can be subdivided into \n",
            "categories for convenience. A coarse division is given below. \n",
            "Text and speech processing[edit] \n",
            "Optical character recognition (OCR) \n",
            "Given an image representing printed text, determine the corresponding text. \n",
            "Speech recognition \n",
            "Given a sound clip of a person or people speaking, determine the textual representation \n",
            "of the speech. This is the opposite of text to speech and is one of the extremely difficult \n",
            "problems colloquially termed \"AI-complete\" (see above). In natural speech there are \n",
            "hardly any pauses between successive words, and thus speech segmentation is a \n",
            "necessary subtask of speech recognition (see below). In most spoken languages, the \n",
            "sounds representing successive letters blend into each other in a process \n",
            "termed coarticulation, so the conversion of the analog signal to discrete characters can \n",
            "be a very difficult process. Also, given that words in the same language are spoken by \n",
            "people with different accents, the speech recognition software must be able to recognize \n",
            "the wide variety of input as being identical to each other in terms of its textual equivalent. \n",
            "Speech segmentation \n",
            "Given a sound clip of a person or people speaking, separate it into words. A subtask \n",
            "of speech recognition and typically grouped with it. \n",
            "Text-to-speech \n",
            "Given a text, transform those units and produce a spoken representation. Text-to-speech \n",
            "can be used to aid the visually impaired.[23] \n",
            "Word segmentation (Tokenization) \n",
            "Separate a chunk of continuous text into separate words. For a language like English, \n",
            "this is fairly trivial, since words are usually separated by spaces. However, some written \n",
            "languages like Chinese, Japanese and Thai do not mark word boundaries in such a \n",
            "fashion, and in those languages text segmentation is a significant task requiring \n",
            "knowledge of the vocabulary and morphology of words in the language. Sometimes this \n",
            "process is also used in cases like bag of words (BOW) creation in data mining. \n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers wikipedia newspaper3k GoogleNews pyvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqBfnlHGmoDq",
        "outputId": "44de3d3b-3afa-4887-a168-bbf719c6e017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: GoogleNews in /usr/local/lib/python3.10/dist-packages (1.6.13)\n",
            "Requirement already satisfied: pyvis in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.2)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (from GoogleNews) (1.2.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.3)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.3)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.0.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser->GoogleNews) (2023.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->GoogleNews) (5.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMVVc3epkQbC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import math\n",
        "import torch\n",
        "import wikipedia\n",
        "from newspaper import Article, ArticleException\n",
        "from GoogleNews import GoogleNews\n",
        "import IPython\n",
        "from pyvis.network import Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DUOJhoVltPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad98994c-7388-4b78-afc5-999d6b4375b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_relations_from_model_output(text):\n",
        "    relations = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
        "    for token in text_replaced.split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        relations.append({\n",
        "            'head': subject.strip(),\n",
        "            'type': relation.strip(),\n",
        "            'tail': object_.strip()\n",
        "        })\n",
        "    return relations"
      ],
      "metadata": {
        "id": "Ek7ykr90nDqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KB():\n",
        "    def __init__(self):\n",
        "        self.relations = []\n",
        "\n",
        "    def are_relations_equal(self, r1, r2):\n",
        "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
        "\n",
        "    def exists_relation(self, r1):\n",
        "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
        "\n",
        "    def add_relation(self, r):\n",
        "        if not self.exists_relation(r):\n",
        "            self.relations.append(r)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"Relations:\")\n",
        "        for r in self.relations:\n",
        "            print(f\"  {r}\")"
      ],
      "metadata": {
        "id": "penJ1-BPnHU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_small_text_to_kb(text, verbose=False):\n",
        "    kb = KB()\n",
        "\n",
        "    # Tokenizer text\n",
        "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n",
        "                            return_tensors='pt')\n",
        "    if verbose:\n",
        "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
        "\n",
        "    # Generate\n",
        "    gen_kwargs = {\n",
        "        \"max_length\": 216,\n",
        "        \"length_penalty\": 0,\n",
        "        \"num_beams\": 3,\n",
        "        \"num_return_sequences\": 3\n",
        "    }\n",
        "    generated_tokens = model.generate(\n",
        "        **model_inputs,\n",
        "        **gen_kwargs,\n",
        "    )\n",
        "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
        "\n",
        "    # create kb\n",
        "    for sentence_pred in decoded_preds:\n",
        "        relations = extract_relations_from_model_output(sentence_pred)\n",
        "        for r in relations:\n",
        "            kb.add_relation(r)\n",
        "\n",
        "    return kb"
      ],
      "metadata": {
        "id": "0dO1S6qjnJE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb = from_small_text_to_kb(processed_text, verbose=True)\n",
        "kb.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyHAiiUInLkR",
        "outputId": "bda930c1-3409-4d59-c686-14461dcce61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num tokens: 512\n",
            "Relations:\n",
            "  {'head': 'Computing Machinery and Intelligence', 'type': 'author', 'tail': 'Alan Turing'}\n",
            "  {'head': 'natural language', 'type': 'studied by', 'tail': 'linguistics'}\n",
            "  {'head': 'linguistics', 'type': 'studies', 'tail': 'natural language'}\n",
            "  {'head': 'Turing test', 'type': 'discoverer or inventor', 'tail': 'Alan Turing'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KB():\n",
        "    def __init__(self):\n",
        "        self.relations = []\n",
        "\n",
        "    def are_relations_equal(self, r1, r2):\n",
        "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
        "\n",
        "    def exists_relation(self, r1):\n",
        "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
        "\n",
        "    def merge_relations(self, r1):\n",
        "        r2 = [r for r in self.relations\n",
        "              if self.are_relations_equal(r1, r)][0]\n",
        "        spans_to_add = [span for span in r1[\"meta\"][\"spans\"]\n",
        "                        if span not in r2[\"meta\"][\"spans\"]]\n",
        "        r2[\"meta\"][\"spans\"] += spans_to_add\n",
        "\n",
        "    def add_relation(self, r):\n",
        "        if not self.exists_relation(r):\n",
        "            self.relations.append(r)\n",
        "        else:\n",
        "            self.merge_relations(r)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"Relations:\")\n",
        "        for r in self.relations:\n",
        "            print(f\"  {r}\")"
      ],
      "metadata": {
        "id": "mVzC78A8nj_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_text_to_kb(text, span_length=128, verbose=False):\n",
        "    # tokenize whole text\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "\n",
        "    # compute span boundaries\n",
        "    num_tokens = len(inputs[\"input_ids\"][0])\n",
        "    if verbose:\n",
        "        print(f\"Input has {num_tokens} tokens\")\n",
        "    num_spans = math.ceil(num_tokens / span_length)\n",
        "    if verbose:\n",
        "        print(f\"Input has {num_spans} spans\")\n",
        "    overlap = math.ceil((num_spans * span_length - num_tokens) /\n",
        "                        max(num_spans - 1, 1))\n",
        "    spans_boundaries = []\n",
        "    start = 0\n",
        "    for i in range(num_spans):\n",
        "        spans_boundaries.append([start + span_length * i,\n",
        "                                 start + span_length * (i + 1)])\n",
        "        start -= overlap\n",
        "    if verbose:\n",
        "        print(f\"Span boundaries are {spans_boundaries}\")\n",
        "\n",
        "    # transform input with spans\n",
        "    tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]]\n",
        "                  for boundary in spans_boundaries]\n",
        "    tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]]\n",
        "                    for boundary in spans_boundaries]\n",
        "    inputs = {\n",
        "        \"input_ids\": torch.stack(tensor_ids),\n",
        "        \"attention_mask\": torch.stack(tensor_masks)\n",
        "    }\n",
        "\n",
        "    # generate relations\n",
        "    num_return_sequences = 3\n",
        "    gen_kwargs = {\n",
        "        \"max_length\": 256,\n",
        "        \"length_penalty\": 0,\n",
        "        \"num_beams\": 3,\n",
        "        \"num_return_sequences\": num_return_sequences\n",
        "    }\n",
        "    generated_tokens = model.generate(\n",
        "        **inputs,\n",
        "        **gen_kwargs,\n",
        "    )\n",
        "\n",
        "    # decode relations\n",
        "    decoded_preds = tokenizer.batch_decode(generated_tokens,\n",
        "                                           skip_special_tokens=False)\n",
        "\n",
        "    # create kb\n",
        "    kb = KB()\n",
        "    i = 0\n",
        "    for sentence_pred in decoded_preds:\n",
        "        current_span_index = i // num_return_sequences\n",
        "        relations = extract_relations_from_model_output(sentence_pred)\n",
        "        for relation in relations:\n",
        "            relation[\"meta\"] = {\n",
        "                \"spans\": [spans_boundaries[current_span_index]]\n",
        "            }\n",
        "            kb.add_relation(relation)\n",
        "        i += 1\n",
        "\n",
        "    return kb"
      ],
      "metadata": {
        "id": "18S9_KZVn0PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kb = from_text_to_kb(processed_text, verbose=True)\n",
        "kb.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bh8GBvpn2DK",
        "outputId": "d4ca66e4-4c48-4dd9-c4fc-989509343cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2770 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input has 2770 tokens\n",
            "Input has 22 spans\n",
            "Span boundaries are [[0, 128], [125, 253], [250, 378], [375, 503], [500, 628], [625, 753], [750, 878], [875, 1003], [1000, 1128], [1125, 1253], [1250, 1378], [1375, 1503], [1500, 1628], [1625, 1753], [1750, 1878], [1875, 2003], [2000, 2128], [2125, 2253], [2250, 2378], [2375, 2503], [2500, 2628], [2625, 2753]]\n",
            "Relations:\n",
            "  {'head': 'natural language', 'type': 'studied by', 'tail': 'linguistics', 'meta': {'spans': [[0, 128]]}}\n",
            "  {'head': 'speech corpora', 'type': 'subclass of', 'tail': 'natural language', 'meta': {'spans': [[0, 128]]}}\n",
            "  {'head': 'Natural language processing', 'type': 'subclass of', 'tail': 'computer', 'meta': {'spans': [[0, 128]]}}\n",
            "  {'head': 'Natural language processing', 'type': 'subclass of', 'tail': 'linguistics', 'meta': {'spans': [[0, 128]]}}\n",
            "  {'head': 'Computing Machinery and Intelligence', 'type': 'author', 'tail': 'Alan Turing', 'meta': {'spans': [[125, 253]]}}\n",
            "  {'head': 'Turing test', 'type': 'named after', 'tail': 'Alan Turing', 'meta': {'spans': [[125, 253]]}}\n",
            "  {'head': 'Georgetown experiment', 'type': 'point in time', 'tail': '1954', 'meta': {'spans': [[250, 378]]}}\n",
            "  {'head': 'Chinese room experiment', 'type': 'named after', 'tail': 'John Searle', 'meta': {'spans': [[250, 378]]}}\n",
            "  {'head': 'Chinese room experiment', 'type': 'discoverer or inventor', 'tail': 'John Searle', 'meta': {'spans': [[250, 378]]}}\n",
            "  {'head': 'ALPAC report', 'type': 'publication date', 'tail': '1966', 'meta': {'spans': [[375, 503]]}}\n",
            "  {'head': 'ALPAC', 'type': 'publication date', 'tail': '1966', 'meta': {'spans': [[375, 503]]}}\n",
            "  {'head': 'ALPAC report in 1966', 'type': 'publication date', 'tail': '1966', 'meta': {'spans': [[375, 503]]}}\n",
            "  {'head': 'SHRDLU', 'type': 'developer', 'tail': 'Joseph Weizenbaum', 'meta': {'spans': [[500, 628]]}}\n",
            "  {'head': 'ELIZA', 'type': 'developer', 'tail': 'Joseph Weizenbaum', 'meta': {'spans': [[500, 628]]}}\n",
            "  {'head': 'MARGIE', 'type': 'publication date', 'tail': '1975', 'meta': {'spans': [[625, 753]]}}\n",
            "  {'head': 'Ross Quillian', 'type': 'field of work', 'tail': 'natural language', 'meta': {'spans': [[625, 753]]}}\n",
            "  {'head': 'MARGIE', 'type': 'developer', 'tail': 'Schank', 'meta': {'spans': [[625, 753]]}}\n",
            "  {'head': 'PARRY', 'type': 'instance of', 'tail': 'chatterbot', 'meta': {'spans': [[750, 878]]}}\n",
            "  {'head': 'PARRY', 'type': 'instance of', 'tail': 'chatterbots', 'meta': {'spans': [[750, 878]]}}\n",
            "  {'head': 'PAM', 'type': 'publication date', 'tail': '1978', 'meta': {'spans': [[750, 878]]}}\n",
            "  {'head': 'generative grammar', 'type': 'subclass of', 'tail': 'morphology', 'meta': {'spans': [[875, 1003]]}}\n",
            "  {'head': 'two-level morphology', 'type': 'subclass of', 'tail': 'morphology', 'meta': {'spans': [[875, 1003]]}}\n",
            "  {'head': 'machine learning', 'type': 'subclass of', 'tail': 'algorithm', 'meta': {'spans': [[1000, 1128]]}}\n",
            "  {'head': 'machine learning', 'type': 'subclass of', 'tail': 'algorithms', 'meta': {'spans': [[1000, 1128]]}}\n",
            "  {'head': 'Chomskyan', 'type': 'field of work', 'tail': 'transformational grammar', 'meta': {'spans': [[1000, 1128]]}}\n",
            "  {'head': 'machine-learning', 'type': 'part of', 'tail': 'language processing', 'meta': {'spans': [[1125, 1253]]}}\n",
            "  {'head': 'language processing', 'type': 'has part', 'tail': 'machine-learning', 'meta': {'spans': [[1125, 1253]]}}\n",
            "  {'head': 'machine-learning', 'type': 'use', 'tail': 'language processing', 'meta': {'spans': [[1125, 1253]]}}\n",
            "  {'head': 'unsupervised', 'type': 'subclass of', 'tail': 'learning', 'meta': {'spans': [[1250, 1378]]}}\n",
            "  {'head': 'data', 'type': 'use', 'tail': 'learning', 'meta': {'spans': [[1250, 1378]]}}\n",
            "  {'head': 'unsupervised', 'type': 'subclass of', 'tail': 'learning from limited amounts of data', 'meta': {'spans': [[1250, 1378]]}}\n",
            "  {'head': 'supervised', 'type': 'subclass of', 'tail': 'learning algorithm', 'meta': {'spans': [[1375, 1503]]}}\n",
            "  {'head': 'semi-supervised', 'type': 'subclass of', 'tail': 'learning algorithm', 'meta': {'spans': [[1375, 1503]]}}\n",
            "  {'head': 'supervised and semi-supervised learning', 'type': 'subclass of', 'tail': 'algorithms', 'meta': {'spans': [[1375, 1503]]}}\n",
            "  {'head': 'supervised', 'type': 'subclass of', 'tail': 'learning algorithms', 'meta': {'spans': [[1375, 1503]]}}\n",
            "  {'head': 'semi-supervised', 'type': 'subclass of', 'tail': 'learning algorithms', 'meta': {'spans': [[1375, 1503]]}}\n",
            "  {'head': 'Tomáš Mikolov', 'type': 'educated at', 'tail': 'Brno University of Technology', 'meta': {'spans': [[1500, 1628]]}}\n",
            "  {'head': 'Tomáš Mikolov', 'type': 'employer', 'tail': 'Brno University of Technology', 'meta': {'spans': [[1500, 1628]]}}\n",
            "  {'head': 'Yoshua Bengio', 'type': 'educated at', 'tail': 'Brno University of Technology', 'meta': {'spans': [[1500, 1628]]}}\n",
            "  {'head': 'deep neural network', 'type': 'use', 'tail': 'machine learning', 'meta': {'spans': [[1625, 1753]]}}\n",
            "  {'head': 'deep neural network', 'type': 'subclass of', 'tail': 'machine learning', 'meta': {'spans': [[1625, 1753]]}}\n",
            "  {'head': 'Word2vec', 'type': 'use', 'tail': 'natural language processing', 'meta': {'spans': [[1625, 1753]]}}\n",
            "  {'head': 'healthcare', 'type': 'facet of', 'tail': 'electronic health', 'meta': {'spans': [[1750, 1878]]}}\n",
            "  {'head': 'health', 'type': 'facet of', 'tail': 'healthcare', 'meta': {'spans': [[1750, 1878]]}}\n",
            "  {'head': 'healthcare', 'type': 'facet of', 'tail': 'health', 'meta': {'spans': [[1750, 1878]]}}\n",
            "  {'head': 'neural network', 'type': 'subclass of', 'tail': 'Machine learning', 'meta': {'spans': [[1875, 2003]]}}\n",
            "  {'head': 'neural networks', 'type': 'subclass of', 'tail': 'Machine learning', 'meta': {'spans': [[1875, 2003]]}}\n",
            "  {'head': 'neural network', 'type': 'part of', 'tail': 'Machine learning', 'meta': {'spans': [[1875, 2003]]}}\n",
            "  {'head': 'probabilistic', 'type': 'subclass of', 'tail': 'language model', 'meta': {'spans': [[2000, 2128]]}}\n",
            "  {'head': 'Probabilistic', 'type': 'subclass of', 'tail': 'language model', 'meta': {'spans': [[2000, 2128]]}}\n",
            "  {'head': 'probabilistic', 'type': 'subclass of', 'tail': 'complexity', 'meta': {'spans': [[2000, 2128]]}}\n",
            "  {'head': 'Apertium', 'type': 'use', 'tail': 'machine translation', 'meta': {'spans': [[2125, 2253]]}}\n",
            "  {'head': 'Apertium system', 'type': 'use', 'tail': 'machine translation', 'meta': {'spans': [[2125, 2253]]}}\n",
            "  {'head': 'Apertium', 'type': 'instance of', 'tail': 'machine translation', 'meta': {'spans': [[2125, 2253]]}}\n",
            "  {'head': 'Optical character recognition', 'type': 'studies', 'tail': 'text', 'meta': {'spans': [[2250, 2378]]}}\n",
            "  {'head': 'Optical character recognition', 'type': 'uses', 'tail': 'text', 'meta': {'spans': [[2250, 2378]]}}\n",
            "  {'head': 'Optical character recognition', 'type': 'part of', 'tail': 'natural language processing', 'meta': {'spans': [[2250, 2378]]}}\n",
            "  {'head': 'speech segmentation', 'type': 'facet of', 'tail': 'speech recognition', 'meta': {'spans': [[2375, 2503]]}}\n",
            "  {'head': 'speech segmentation', 'type': 'part of', 'tail': 'speech recognition', 'meta': {'spans': [[2375, 2503]]}}\n",
            "  {'head': 'speech segmentation', 'type': 'subclass of', 'tail': 'speech recognition', 'meta': {'spans': [[2375, 2503]]}}\n",
            "  {'head': 'Speech segmentation', 'type': 'facet of', 'tail': 'speech recognition', 'meta': {'spans': [[2500, 2628]]}}\n",
            "  {'head': 'Speech segmentation', 'type': 'subclass of', 'tail': 'speech recognition', 'meta': {'spans': [[2500, 2628]]}}\n",
            "  {'head': 'Speech segmentation', 'type': 'part of', 'tail': 'speech recognition', 'meta': {'spans': [[2500, 2628]]}}\n",
            "  {'head': 'English', 'type': 'instance of', 'tail': 'spoken representation', 'meta': {'spans': [[2625, 2753]]}}\n",
            "  {'head': 'morphology', 'type': 'part of', 'tail': 'vocabulary', 'meta': {'spans': [[2625, 2753]]}}\n",
            "  {'head': 'English', 'type': 'has parts of the class', 'tail': 'words', 'meta': {'spans': [[2625, 2753]]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py2neo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXrfzUBIb1Ej",
        "outputId": "6995872f-1e0c-4c8e-a034-6ef9c0a52102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py2neo in /usr/local/lib/python3.10/dist-packages (2021.2.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from py2neo) (2024.2.2)\n",
            "Requirement already satisfied: interchange~=2021.0.4 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2021.0.4)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from py2neo) (24.0)\n",
            "Requirement already satisfied: pansi>=2020.7.3 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2020.7.3)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.16.1)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.16.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.0.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from interchange~=2021.0.4->py2neo) (2023.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_rdf(kb):\n",
        "    rdf_triples = []\n",
        "    for relation in kb.relations:\n",
        "        subject = relation['head']\n",
        "        subject = subject.replace(\"'\",\"\")\n",
        "        predicate = relation['type']\n",
        "        predicate = predicate.replace(\" \", \"_\")\n",
        "        object_ = relation['tail']\n",
        "        object_ = object_.replace(\"'\",\"\")\n",
        "        rdf_triples.append((subject, predicate, object_))\n",
        "    return rdf_triples\n",
        "\n",
        "# Convert KB relations to RDF triples\n",
        "rdf_triples = convert_to_rdf(kb)"
      ],
      "metadata": {
        "id": "Z7n33LTiqRm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdf_triples"
      ],
      "metadata": {
        "id": "PbGsGsHGVPMC",
        "outputId": "7ab53d9c-a758-4d04-aa3f-a48344cfdbbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('natural language', 'studied_by', 'linguistics'),\n",
              " ('speech corpora', 'subclass_of', 'natural language'),\n",
              " ('Natural language processing', 'subclass_of', 'computer'),\n",
              " ('Natural language processing', 'subclass_of', 'linguistics'),\n",
              " ('Computing Machinery and Intelligence', 'author', 'Alan Turing'),\n",
              " ('Turing test', 'named_after', 'Alan Turing'),\n",
              " ('Georgetown experiment', 'point_in_time', '1954'),\n",
              " ('Chinese room experiment', 'named_after', 'John Searle'),\n",
              " ('Chinese room experiment', 'discoverer_or_inventor', 'John Searle'),\n",
              " ('ALPAC report', 'publication_date', '1966'),\n",
              " ('ALPAC', 'publication_date', '1966'),\n",
              " ('ALPAC report in 1966', 'publication_date', '1966'),\n",
              " ('SHRDLU', 'developer', 'Joseph Weizenbaum'),\n",
              " ('ELIZA', 'developer', 'Joseph Weizenbaum'),\n",
              " ('MARGIE', 'publication_date', '1975'),\n",
              " ('Ross Quillian', 'field_of_work', 'natural language'),\n",
              " ('MARGIE', 'developer', 'Schank'),\n",
              " ('PARRY', 'instance_of', 'chatterbot'),\n",
              " ('PARRY', 'instance_of', 'chatterbots'),\n",
              " ('PAM', 'publication_date', '1978'),\n",
              " ('generative grammar', 'subclass_of', 'morphology'),\n",
              " ('two-level morphology', 'subclass_of', 'morphology'),\n",
              " ('machine learning', 'subclass_of', 'algorithm'),\n",
              " ('machine learning', 'subclass_of', 'algorithms'),\n",
              " ('Chomskyan', 'field_of_work', 'transformational grammar'),\n",
              " ('machine-learning', 'part_of', 'language processing'),\n",
              " ('language processing', 'has_part', 'machine-learning'),\n",
              " ('machine-learning', 'use', 'language processing'),\n",
              " ('unsupervised', 'subclass_of', 'learning'),\n",
              " ('data', 'use', 'learning'),\n",
              " ('unsupervised', 'subclass_of', 'learning from limited amounts of data'),\n",
              " ('supervised', 'subclass_of', 'learning algorithm'),\n",
              " ('semi-supervised', 'subclass_of', 'learning algorithm'),\n",
              " ('supervised and semi-supervised learning', 'subclass_of', 'algorithms'),\n",
              " ('supervised', 'subclass_of', 'learning algorithms'),\n",
              " ('semi-supervised', 'subclass_of', 'learning algorithms'),\n",
              " ('Tomáš Mikolov', 'educated_at', 'Brno University of Technology'),\n",
              " ('Tomáš Mikolov', 'employer', 'Brno University of Technology'),\n",
              " ('Yoshua Bengio', 'educated_at', 'Brno University of Technology'),\n",
              " ('deep neural network', 'use', 'machine learning'),\n",
              " ('deep neural network', 'subclass_of', 'machine learning'),\n",
              " ('Word2vec', 'use', 'natural language processing'),\n",
              " ('healthcare', 'facet_of', 'electronic health'),\n",
              " ('health', 'facet_of', 'healthcare'),\n",
              " ('healthcare', 'facet_of', 'health'),\n",
              " ('neural network', 'subclass_of', 'Machine learning'),\n",
              " ('neural networks', 'subclass_of', 'Machine learning'),\n",
              " ('neural network', 'part_of', 'Machine learning'),\n",
              " ('probabilistic', 'subclass_of', 'language model'),\n",
              " ('Probabilistic', 'subclass_of', 'language model'),\n",
              " ('probabilistic', 'subclass_of', 'complexity'),\n",
              " ('Apertium', 'use', 'machine translation'),\n",
              " ('Apertium system', 'use', 'machine translation'),\n",
              " ('Apertium', 'instance_of', 'machine translation'),\n",
              " ('Optical character recognition', 'studies', 'text'),\n",
              " ('Optical character recognition', 'uses', 'text'),\n",
              " ('Optical character recognition', 'part_of', 'natural language processing'),\n",
              " ('speech segmentation', 'facet_of', 'speech recognition'),\n",
              " ('speech segmentation', 'part_of', 'speech recognition'),\n",
              " ('speech segmentation', 'subclass_of', 'speech recognition'),\n",
              " ('Speech segmentation', 'facet_of', 'speech recognition'),\n",
              " ('Speech segmentation', 'subclass_of', 'speech recognition'),\n",
              " ('Speech segmentation', 'part_of', 'speech recognition'),\n",
              " ('English', 'instance_of', 'spoken representation'),\n",
              " ('morphology', 'part_of', 'vocabulary'),\n",
              " ('English', 'has_parts_of_the_class', 'words')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neo4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_i5iYaD_DLr",
        "outputId": "9fc8253e-44b6-40c0-e36d-9fd3f527460a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.18.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import networkx as nx\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def construct_graph_from_rdf(rdf_triples):\n",
        "#     # Create a directed graph\n",
        "#     G = nx.DiGraph()\n",
        "\n",
        "#     # Iterate through RDF triples and add edges to the graph\n",
        "#     for triple in rdf_triples:\n",
        "#         subject, predicate, object_ = triple\n",
        "#         # Add edges to the graph\n",
        "#         G.add_edge(subject, object_, label=predicate)\n",
        "\n",
        "#     return G\n",
        "\n",
        "# # Construct the graph from RDF triples\n",
        "# graph = construct_graph_from_rdf(rdf_triples)\n",
        "\n",
        "# # Draw the graph\n",
        "# plt.figure(figsize=(16, 16))\n",
        "# pos = nx.spring_layout(graph, seed=42)  # Positions for all nodes\n",
        "# nx.draw(graph, pos, with_labels=True, node_size=2000, node_color=\"skyblue\", font_size=12, font_weight=\"bold\", arrowsize=20)\n",
        "# edge_labels = nx.get_edge_attributes(graph, \"label\")\n",
        "# nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_color=\"red\")\n",
        "\n",
        "# # Show the graph\n",
        "# plt.title(\"Knowledge Graph\")\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "OXyNwXTA_gaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from py2neo import Graph"
      ],
      "metadata": {
        "id": "RG16kPt3FvMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install neo4j-driver"
      ],
      "metadata": {
        "id": "VaOTUhB7FyF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22327f5e-4647-4c14-d411-d578d2c08b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j-driver in /usr/local/lib/python3.10/dist-packages (5.18.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j-driver) (2023.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python3 example.py\n",
        "\n",
        "from neo4j import GraphDatabase, basic_auth\n",
        "\n",
        "driver = GraphDatabase.driver(\n",
        "  \"bolt://100.27.33.53:7687\",\n",
        "  auth=basic_auth(\"neo4j\", \"basements-injector-welder\"))\n",
        "\n",
        "# cypher_query = '''\n",
        "# MATCH (n)\n",
        "# RETURN COUNT(n) AS count\n",
        "# LIMIT $limit\n",
        "# '''\n",
        "\n",
        "# with driver.session(database=\"neo4j\") as session:\n",
        "#   results = session.read_transaction(\n",
        "#     lambda tx: tx.run(cypher_query, limit=10).data())\n",
        "#   for record in results:\n",
        "#     print(record['count'])\n",
        "\n",
        "def execute_query(tx, query):\n",
        "    tx.run(query)\n",
        "\n",
        "# Add RDF triples to Neo4j\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    for subject, predicate, object_ in rdf_triples:\n",
        "        # Define Cypher query to add RDF triple\n",
        "        predicate = predicate.replace(\" \", \"_\")\n",
        "        query = (\n",
        "            f\"MERGE (s:Resource {{name: '{subject}'}})\"\n",
        "            f\"MERGE (o:Resource {{name: '{object_}'}})\"\n",
        "            f\"MERGE (s)-[:`{predicate}`]->(o)\"\n",
        "            )\n",
        "        # Execute the query\n",
        "        # results = session.execute_read(\n",
        "        #     lambda tx: tx.run(query).data())\n",
        "        session.execute_write(execute_query, query)\n",
        "\n",
        "driver.close()\n"
      ],
      "metadata": {
        "id": "jObJmVLbF3ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.text_splitter import CharacterTextSplitter\n",
        "# from langchain.docstore.document import Document\n",
        "# from langchain.chains.summarize import load_summarize_chain\n",
        "# from langchain import PromptTemplate\n",
        "# from langchain.llms import CTransformers\n",
        "# from langchain.callbacks.manager import CallbackManager\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "\n",
        "\n",
        "# # this function is responsible for splitting the data into smaller chunks and convert the data in document format\n",
        "# def chunks_and_document(txt):\n",
        "\n",
        "#     text_splitter = CharacterTextSplitter() # text splitter method by default it has chunk_size = 200 and chunk_overlap = 200\n",
        "#     texts = text_splitter.split_text(txt) # split the text into smaller chunks\n",
        "#     docs = [Document(page_content=t) for t in texts] # convert the splitted chunks into document format\n",
        "\n",
        "#     return docs\n",
        "\n",
        "# # Loading the Llama 2's LLM\n",
        "# def load_llm():\n",
        "#     # We instantiate the callback with a streaming stdout handler\n",
        "#     callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "#     # loading the LLM model\n",
        "#     # This open source model can be downloaded from here\n",
        "#     # Their are multiple models available just replace it in place of model and try it.\n",
        "#     llm = CTransformers(\n",
        "#         model=r\"C:\\Users\\venky\\OneDrive\\Desktop\\Learnings\\Projects\\Llama-2-Streamlit-Chatbot\\llama-2-7b-chat.ggmlv3.q2_K.bin\",\n",
        "#         model_type=\"llama\",\n",
        "#          max_new_tokens = 512,\n",
        "#         temperature = 0.5   )\n",
        "\n",
        "#     return llm\n",
        "\n",
        "# # this functions is used for applying the llm model with our document\n",
        "# def chains_and_response(docs):\n",
        "\n",
        "#     llm = load_llm()\n",
        "#     chain = load_summarize_chain(llm,chain_type='map_reduce')\n",
        "\n",
        "#     return chain.run(docs)\n",
        "\n",
        "# # Form to accept user's text input for summarization\n",
        "# result = []\n",
        "# with st.form('summarize_form', clear_on_submit=True):\n",
        "#     submitted = st.form_submit_button('Submit')\n",
        "#     #if submitted and openai_api_key.startswith('sk-'):\n",
        "#     if submitted:\n",
        "#         with st.spinner('Calculating...'):\n",
        "#             docs = chunks_and_document(txt_input)\n",
        "#             response = chains_and_response(docs)\n",
        "#             result.append(response)"
      ],
      "metadata": {
        "id": "fGQcnPQEF75I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pytextrank\n",
        "# !python3 -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "btkkXXWWbGC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# import pytextrank\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_lg\")\n",
        "# nlp.add_pipe(\"textrank\")\n",
        "\n",
        "# print('Original Document Size:',len(processed_text))\n",
        "# doc = nlp(processed_text)\n",
        "\n",
        "# for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
        "#     print(sent)\n",
        "#     print('Summary Length:',len(sent))"
      ],
      "metadata": {
        "id": "PWWREftZaw54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhMU58nfc_sZ",
        "outputId": "ffd6065e-0281-42bd-f397-e13f6d1dab49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample user question\n",
        "user_question = \"Natural Language processing is studied by whom?\"\n",
        "\n",
        "# Tokenize the user question\n",
        "tokens = word_tokenize(user_question)\n",
        "\n",
        "# Part-of-speech (POS) tagging\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "# Named entity recognition (NER)\n",
        "named_entities = nltk.ne_chunk(tagged_tokens)\n",
        "\n",
        "# Identify entities and relationships\n",
        "entities = []\n",
        "relationships = []\n",
        "\n",
        "for entity in named_entities:\n",
        "    if isinstance(entity, nltk.Tree):\n",
        "        entities.append(\" \".join([word for word, tag in entity.leaves()]))\n",
        "    else:\n",
        "        word, tag = entity\n",
        "        if tag.startswith('NN'):  # Consider only nouns\n",
        "            entities.append(word)\n",
        "\n",
        "# Extract constraints\n",
        "constraints = []\n",
        "for i, (word, tag) in enumerate(tagged_tokens):\n",
        "    if word.lower() == 'after' and i < len(tagged_tokens) - 1:\n",
        "        next_word, next_tag = tagged_tokens[i + 1]\n",
        "        if next_tag == 'CD':  # If the next word is a cardinal number (e.g., '2019')\n",
        "            constraints.append(next_word)\n",
        "\n",
        "print(\"Entities:\", entities)\n",
        "print(\"Constraints:\", constraints)\n"
      ],
      "metadata": {
        "id": "KBZZH3f7a6B9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838ae953-1957-4155-9d5b-e4d04b5f5a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities: ['Language', 'processing']\n",
            "Constraints: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXrDNZWrcp-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}